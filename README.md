# ğŸ¬ Movie Recommendation System

A production-ready content-based movie recommendation system with semantic understanding and fast similarity search.

## âœ¨ Features

- **Hybrid Recommendations** - Combines semantic (BERT) and metadata (TF-IDF) approaches
- **Fast Search** - FAISS-powered similarity search for scalability
- **Smart Caching** - Pre-computed recommendations for instant lookups
- **MMR Diversity** - Maximal Marginal Relevance for diverse recommendations
- **Learned Fusion** - ML-based optimization of signal combination
- **Configurable** - Easy parameter tuning via YAML config
- **Evaluation Metrics** - Objective quality measurement

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Pipeline

```bash
python -m scripts.run_pipeline \
  --movies datasets/tmdb_5000_movies.csv \
  --credits datasets/tmdb_5000_credits.csv \
  --outdir outputs \
  --use_cache
```

### 3. Get Instant Recommendations

```python
from scripts.cache import RecommendationCache

# Load pre-computed cache
cache = RecommendationCache('cache')
cache.load()

# Get recommendations (< 1ms)
recs = cache.get_recommendations('Inception', k=10)
for i, (title, score) in enumerate(recs, 1):
    print(f"{i}. {title} ({score:.4f})")
```

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| **Pipeline Execution** | ~10s (with cache) |
| **Recommendation Lookup** | < 1ms (cached) |
| **Memory Usage** | ~20 MB (cached) |
| **Scalability** | Millions of movies |

## âš™ï¸ Configuration

Edit `config.yaml` to customize:

```yaml
model:
  name: "all-MiniLM-L6-v2"  # Sentence transformer model
  batch_size: 32

similarity:
  alpha: 0.5  # BERT vs TF-IDF weight (0-1)
  use_faiss: true

recommendations:
  default_k: 10
  cache_top_k: 100
```

## ğŸ¯ Command-Line Options

```bash
# Basic usage
python -m scripts.run_pipeline \
  --movies <path> \
  --credits <path> \
  --outdir <path>

# With caching (recommended)
python -m scripts.run_pipeline \
  --movies <path> \
  --credits <path> \
  --outdir <path> \
  --use_cache

# With FAISS for large datasets
python -m scripts.run_pipeline \
  --movies <path> \
  --credits <path> \
  --outdir <path> \
  --use_faiss \
  --use_cache

# With evaluation
python -m scripts.run_pipeline \
  --movies <path> \
  --credits <path> \
  --outdir <path> \
  --evaluate

# Tune alpha parameter
python -m scripts.run_pipeline \
  --movies <path> \
  --credits <path> \
  --outdir <path> \
  --skip_embeddings \
  --alpha 0.7
```

## ğŸ“ Project Structure

```
paper/
â”œâ”€â”€ config.yaml                # Configuration file
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ datasets/                  # Movie data
â”‚   â”œâ”€â”€ tmdb_5000_movies.csv
â”‚   â””â”€â”€ tmdb_5000_credits.csv
â”œâ”€â”€ scripts/                   # Source code
â”‚   â”œâ”€â”€ run_pipeline.py       # Main pipeline
â”‚   â”œâ”€â”€ embeddings.py         # BERT/sentence-transformers
â”‚   â”œâ”€â”€ similarity.py         # Similarity computation
â”‚   â”œâ”€â”€ faiss_similarity.py   # FAISS integration
â”‚   â”œâ”€â”€ cache.py              # Caching system
â”‚   â”œâ”€â”€ evaluation.py         # Metrics
â”‚   â”œâ”€â”€ recommend.py          # Recommendation logic
â”‚   â”œâ”€â”€ utils.py              # Data preprocessing
â”‚   â””â”€â”€ config.py             # Config loader
â”œâ”€â”€ outputs/                   # Generated files
â”‚   â”œâ”€â”€ preprocessed.pkl
â”‚   â”œâ”€â”€ bert_embeddings.npy
â”‚   â”œâ”€â”€ sim_combined.npy
â”‚   â”œâ”€â”€ faiss_index.bin
â”‚   â””â”€â”€ recommendations_*.json
â””â”€â”€ cache/                     # Pre-computed cache
    â””â”€â”€ recommendation_cache.pkl
```

## ğŸ”§ How It Works

### 1. Data Preprocessing
- Merges movie metadata and credits
- Extracts features: genres, keywords, cast, director
- Creates enriched text: `"Title. Overview. Genres: ... Keywords: ..."`

### 2. Feature Extraction
- **Semantic:** BERT embeddings from enriched text (384-dim)
- **Metadata:** TF-IDF vectors from tags (50K-dim)

### 3. Similarity Computation
- **BERT:** Cosine similarity on embeddings
- **TF-IDF:** Cosine similarity on sparse vectors
- **Hybrid:** Weighted combination (Î± Ã— BERT + (1-Î±) Ã— TF-IDF)

### 4. Recommendations
- Find top-K most similar movies
- Optional: Pre-compute and cache for instant lookups

## ğŸ“ˆ Example Results

**Recommendations for "The Dark Knight":**
1. The Dark Knight Rises (2012) - 0.5909
2. Batman Begins (2005) - 0.4745
3. Batman (1989) - 0.4516
4. Batman v Superman: Dawn of Justice (2016) - 0.4275
5. Batman Returns (1992) - 0.4093

**Recommendations for "Inception":**
1. Subconscious (2015) - 0.2984
2. In Dreams (1999) - 0.2834
3. Unknown (2011) - 0.2808
4. Looper (2012) - 0.2739
5. Frailty (2001) - 0.2660

## ğŸ§ª Evaluation Metrics

Run with `--evaluate` to get:
- **Diversity** - Variety of recommendations
- **Coverage** - % of catalog recommended
- **Intra-list Diversity** - Difference within recommendation lists
- **Novelty** - Frequency of recommending less popular items

## ğŸ“ Technical Details

### Models Used
- **Default:** `all-MiniLM-L6-v2` (22M params, 384-dim)
- **Alternative:** `all-mpnet-base-v2` (110M params, 768-dim, higher quality)

### Algorithms
- **Similarity:** Cosine similarity on L2-normalized vectors
- **Search:** FAISS (Facebook AI Similarity Search) for scalability
- **Fusion:** Linear combination of semantic + metadata signals

### Scalability
- **FAISS:** O(log N) search vs O(N) naive scan
- **Caching:** O(1) lookup vs O(N) computation
- **Memory:** ~20 MB (cached) vs ~185 MB (full matrices)

## ğŸ“š Documentation

- **Quick Start:** This README
- **Phase 1 Guide:** `PHASE1_IMPROVEMENTS.md`
- **Phase 2 Guide:** `PHASE2_GUIDE.md`
- **Quick Reference:** `QUICK_REFERENCE.md`

## ğŸš¦ Dependencies

Core:
- `numpy`, `pandas`, `scipy`, `scikit-learn`
- `torch`, `sentence-transformers`
- `joblib`, `pyyaml`

Optional:
- `faiss-cpu` (or `faiss-gpu`) - For fast similarity search
- `kagglehub` - For dataset download

## ğŸ¤ Contributing

This is a research/educational project. Feel free to:
- Experiment with different models
- Try different alpha values
- Add new evaluation metrics
- Implement Phase 3 features (MMR, learned fusion, API)

## ğŸ“ License

This project uses the TMDB 5000 Movie Dataset for educational purposes.

## ğŸ¯ Advanced Features (Phase 3)

### MMR Diversity
```bash
# Get diverse recommendations
python -m scripts.run_pipeline \
  --movies <path> \
  --credits <path> \
  --outdir <path> \
  --use_mmr \
  --mmr_lambda 0.5
```

### Learned Fusion
```bash
# ML-optimized signal combination
python -m scripts.run_pipeline \
  --movies <path> \
  --credits <path> \
  --outdir <path> \
  --use_learned_fusion
```

### Combined (Best Quality)
```bash
python -m scripts.run_pipeline \
  --movies <path> \
  --credits <path> \
  --outdir <path> \
  --use_mmr \
  --use_learned_fusion
```

## ğŸ”œ Future Enhancements (Phase 4)

- [ ] Contextual filtering (genre, year, rating)
- [ ] REST API with FastAPI
- [ ] A/B testing framework
- [ ] User preference learning
- [ ] Real-time personalization

---

**Built with:** Python, BERT, FAISS, sentence-transformers  
**Dataset:** TMDB 5000 Movie Dataset  
**Status:** Production Ready âœ…

